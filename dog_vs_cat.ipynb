{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog vs Cat Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Start from [Check Point 1](#Check Point 1) if data have beeen preprocessed and saved to disk when coming back to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2, os, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('All modules are imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Explore the Data\n",
    "#### Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image directoires\n",
    "TRAIN_DIR = 'input/train/'\n",
    "TEST_DIR = 'input/test/'\n",
    "\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)]\n",
    "test_images = [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "# print out statistics of the dataset and show example images\n",
    "print(\"Stats of Dog_vs_Cat Train Dataset\")\n",
    "print(\"Total Images: {}\".format(len(train_images)))\n",
    "print(\"Dog:          {}\".format(len([i for i in train_images if 'dog' in i ])))\n",
    "print(\"Cat:          {}\".format(len([i for i in train_images if 'cat' in i ])))\n",
    "print('')\n",
    "print('Stats of Dog_vs_Cat Test Dataset')\n",
    "print('Total Images: {}'.format(len(test_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "image_id_list = ['cat.2341', 'cat.1623', 'dog.6128']\n",
    "\n",
    "for image_id in image_id_list:\n",
    "    image_example = cv2.imread('input/train/{}.jpg'.format(image_id), cv2.IMREAD_COLOR)\n",
    "    print(\"Example of Train Image: {}.jpg\".format(image_id))\n",
    "    print(\"Image - Shape: {}\".format(image_example.shape))\n",
    "    print(\"Image - Min Value: {}, Max Value: {}\".format(image_example.min(), image_example.max()))\n",
    "    print(\"Label - {}\".format('dog' if 'dog' in 'input/train/{}.jpg'.format(image_id) else 'cat'))\n",
    "    display(Image.open('input/train/{}.jpg'.format(image_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Preprocessing Functions\n",
    "#### Resize images\n",
    "The images have various sizes. So the images need to be resized to same size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(list_img_dir, dsize=(64, 64, 3)):\n",
    "    \"\"\"\n",
    "    Resizes images to the specified size, and return\n",
    "    resized image pixels in shape: [imgs_num, row, column, channel]\n",
    "    \n",
    "    param list_img_dir: list of image directories.\n",
    "    param dsize: (row, column, channel).\n",
    "    \"\"\"\n",
    "    # Define specified sizes\n",
    "    ROW = dsize[0]\n",
    "    COL = dsize[1]\n",
    "    CHANNEL = dsize[2]\n",
    "    \n",
    "    features = []\n",
    "    # Resize the images\n",
    "    for img in tqdm(list_img_dir, desc='Resize Images', unit='Images'):\n",
    "        img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, dsize=(ROW, COL))\n",
    "        features.append(img)\n",
    "    \n",
    "    features = np.array(features, dtype=np.uint8)\n",
    "    \n",
    "    print('Images are resized to: {}'.format(dsize))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the resized example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the resized image\n",
    "example_features = resize_image(['input/train/{}.jpg'.format(j) for j in image_id_list], dsize=(224, 224, 3))\n",
    "\n",
    "#Show resized example images\n",
    "for i, image_id in enumerate([int(j.split('.')[1]) for j in image_id_list]):\n",
    "    print('Resized Example Images')\n",
    "    print('')\n",
    "    print(\"Example of Train Image: {}.jpg\".format(image_id))\n",
    "    print(\"Image - Shape: {}\".format(example_features[i].shape))\n",
    "    print(\"Image - Min Value: {}, Max Value: {}\".format(example_features[i].min(), example_features[i].max()))\n",
    "    print(\"Label - {}\".format('dog' if 'dog' in image_id_list[i] else 'cat'))\n",
    "    display(Image.fromarray(example_features[i]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the image data with Min-Max scaling to a range of [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_image(features):\n",
    "    \"\"\"\n",
    "    Normalize a list of image pixels with Min-Max scaling to a range of [0.0, 1.0].\n",
    "    param featurs: image pixels in shape: [imgs_num, row, column, channel]\n",
    "    return: Normalized image pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    features = (features-0)/(255-0)\n",
    "    print('Features are normalized to [0.0, 1.0] with Min-Max Scaling.')\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(list_img_dir):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels.\n",
    "    Return a one-hot enncoded vector for each label.\n",
    "    \"\"\"\n",
    "    labels = [[0, 1] if 'cat' in i else [1, 0] for i in list_img_dir]\n",
    "    labels = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    print('Labels are One-Hot Encoded.')\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_id(list_img_dir):\n",
    "    \"\"\"\n",
    "    Record ids for test data.\n",
    "    \"\"\"\n",
    "    ids = [int(i.split('/')[2].split('.')[0]) for i in list_img_dir]\n",
    "    \n",
    "    print('Test Image Ids are obtained.')\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(features, labels, filename):\n",
    "    \"\"\"\n",
    "    Save Train/Validation Data to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    pickle_file = filename\n",
    "    if not os.path.isfile(pickle_file):\n",
    "        print('Saving data to pickle file...')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as f:\n",
    "                if 'train' in filename:\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            'train_features': features,\n",
    "                            'train_labels': labels\n",
    "                        }, f, pickle.HIGHEST_PROTOCOL)\n",
    "                elif 'validation' in filename:\n",
    "                    pickle.dump(\n",
    "                        {\n",
    "                            'valid_features': features,\n",
    "                            'valid_labels': labels\n",
    "                        }, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    print('Features and labels are cached in pickle file: {}.'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_data(features, ids, filename):\n",
    "    \"\"\"\n",
    "    Save Test Data to disk.\n",
    "    \"\"\"\n",
    "    pickle_file = filename\n",
    "    if not os.path.isfile(pickle_file):\n",
    "        print('Saving data to pickle file...')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as f:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        'test_features': features,\n",
    "                        'test_ids': ids\n",
    "                    }, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    print('Features and IDs are cached in pickle file: {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocessing Images\n",
    "\n",
    "* Train/Validation Set: resize -> normalize -> one-hot encode -> save\n",
    "* Test Set: resize  -> normalize -> save\n",
    "\n",
    "Also be noted that the datasets will be split into chunks to run the preprocessing steps. \n",
    "\n",
    "We can't run the preprocessing steps for all image data at the same time, because this will run out of memory resources when image size is huge.\n",
    "\n",
    "The computer is used has 32 GB RAM, and it succeeds to process all train images with (64x64x3) at the same time, but fails with (224x224x3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train dataset (shuffle by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train validation split\n",
    "# Validation set size is 1000, 4% of total train images\n",
    "train_imgs, val_imgs = train_test_split(train_images, test_size=0.04, random_state=42)\n",
    "\n",
    "print('Train Dataset is split, and shuffled randomly.')\n",
    "print('Train Image:            {}'.format(len(train_imgs)))\n",
    "print('Validation Image:       {}'.format(len(val_imgs)))\n",
    "print('Train/Validation Ratio: {}/{}'.format(len(train_imgs)/len(train_images), len(val_imgs)/len(train_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess and save data in chunks due to memory limit\n",
    "chunk_size = 4096\n",
    "image_size = (64, 64, 3)\n",
    "\n",
    "for img_set_num, img_set in enumerate([train_imgs, val_imgs, test_images]):\n",
    "    for i, start in enumerate(range(0, len(img_set), chunk_size)):\n",
    "        # Set flags for feature engineering. This will prevent you from skipping on important step.\n",
    "        is_features_resize = False\n",
    "        is_features_normal = False\n",
    "        is_labels_encod = False\n",
    "        \n",
    "        if img_set_num == 0: #Train Set\n",
    "            pickle_file = 'preprocess_train_{}_{}_chunk_{}.p'.format(image_size[0], image_size[1], i)\n",
    "        elif img_set_num == 1: # Validation Set\n",
    "            pickle_file = 'preprocess_validation_{}_{}_chunk_{}.p'.format(image_size[0], image_size[1], i)\n",
    "        else: # Test Set\n",
    "            pickle_file = 'preprocess_test_{}_{}_chunk_{}.p'.format(image_size[0], image_size[1], i)\n",
    "        \n",
    "        if not os.path.isfile(pickle_file):\n",
    "            end = min(start + chunk_size, len(img_set))\n",
    "            images = img_set[start: end]\n",
    "            \n",
    "            features = resize_image(images, image_size)\n",
    "            is_features_resize = True\n",
    "            \n",
    "            features = norm_image(features)\n",
    "            is_features_normal = True\n",
    "            \n",
    "            if img_set_num == 0 or img_set_num == 1: #Train and Validation Set\n",
    "                labels = one_hot_encode(images)\n",
    "                is_labels_encod = True\n",
    "                save_data(features, labels, pickle_file)\n",
    "                \n",
    "                assert is_features_resize, 'You skipped the step to resize the features'\n",
    "                assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "                assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "            \n",
    "            else: # Test Set\n",
    "                ids = test_id(images)\n",
    "                save_test_data(features, ids, pickle_file)\n",
    "                \n",
    "                assert is_features_resize, 'You skipped the step to resize the features'\n",
    "                assert is_features_normal, 'You skipped the step to normalize the features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Check Point 1'></a>\n",
    "## Check Point 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If come back to this notebook or have to restart the notebook, start from here. The preprocessed data have been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules are imported.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Load the modules\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print('All modules are imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload preprocessed train and validation dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_train_images = [i for i in os.listdir() if 'preprocess_train' in i]\n",
    "train_feature_chunk = []\n",
    "train_label_chunk = []\n",
    "for preprocess_train in preprocess_train_images:\n",
    "    with open(preprocess_train, 'rb') as f:\n",
    "        pickle_chunk_data=pickle.load(f)\n",
    "        train_feature_chunk.append(pickle_chunk_data['train_features'])\n",
    "        train_label_chunk.append(pickle_chunk_data['train_labels'])\n",
    "        del pickle_chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_valid_images = [i for i in os.listdir() if 'preprocess_validation' in i]\n",
    "val_feature_chunk = []\n",
    "val_label_chunk = []\n",
    "for preprocess_valid in preprocess_valid_images:\n",
    "    with open(preprocess_valid, 'rb') as f:\n",
    "        pickle_chunk_data=pickle.load(f)\n",
    "        val_feature_chunk.append(pickle_chunk_data['valid_features'])\n",
    "        val_label_chunk.append(pickle_chunk_data['valid_labels'])\n",
    "        del pickle_chunk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides, conv_padding='SAME'):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    :param conv_padding: 'SAME' or 'VALID'\n",
    "    :param pool_padding: 'SAME' or 'VALID\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    weight =  tf.Variable(tf.truncated_normal(conv_ksize+(x_tensor.shape[-1].value,)+(conv_num_outputs,),\n",
    "                                           stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    x = tf.nn.conv2d(x_tensor, weight, strides=(1,)+conv_strides+(1,), padding=conv_padding)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, conv_padding='SAME', pool_padding='SAME'):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    :param conv_padding: 'SAME' or 'VALID'\n",
    "    :param pool_padding: 'SAME' or 'VALID\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    weight =  tf.Variable(tf.truncated_normal(conv_ksize+(x_tensor.shape[-1].value,)+(conv_num_outputs,),\n",
    "                                           stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    x = tf.nn.conv2d(x_tensor, weight, strides=(1,)+conv_strides+(1,), padding=conv_padding)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.max_pool(x, ksize=((1,)+ pool_ksize + (1,)),\n",
    "                      strides=((1,)+ pool_strides + (1,)),\n",
    "                      padding=pool_padding)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    flatten_x = tf.reshape(x_tensor, \n",
    "                           shape=[tf.shape(x_tensor)[0], x_tensor.shape[1].value*x_tensor.shape[2].value*x_tensor.shape[3].value])\n",
    "    return flatten_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weight = tf.Variable(tf.truncated_normal((x_tensor.shape[-1].value, num_outputs),\n",
    "                                            stddev = 0.05))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    fc_x = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    fc_x = tf.nn.relu(fc_x)\n",
    "    \n",
    "    return fc_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.shape[-1].value,num_outputs],\n",
    "                                            stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, conv_padding, pool_padding)\n",
    "    #conv = conv2d(x, 64, (3, 3), (1, 1), conv_padding='SAME')\n",
    "    conv = conv2d_maxpool(x, 64, (3, 3), (1, 1), (2, 2), (2, 2), conv_padding='SAME', pool_padding='VALID')\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    #conv = conv2d(conv, 128, (3, 3), (1, 1), conv_padding='SAME')\n",
    "    conv = conv2d_maxpool(conv, 128, (3, 3), (1, 1), (2, 2), (2, 2), conv_padding='SAME', pool_padding='VALID')\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    conv = conv2d_maxpool(conv, 128, (3, 3), (1, 1), (2, 2), (2, 2), conv_padding='SAME', pool_padding='VALID')\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    #conv = conv2d(conv, 256, (3, 3), (1, 1), conv_padding='SAME')\n",
    "    conv = conv2d_maxpool(conv, 256, (3, 3), (1, 1), (2, 2), (2, 2), conv_padding='SAME', pool_padding='VALID')\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    #conv = conv2d(conv, 512, (3, 3), (1, 1), conv_padding='SAME')\n",
    "    conv = conv2d_maxpool(conv, 512, (3, 3), (1, 1), (2, 2), (2, 2), conv_padding='SAME', pool_padding='VALID')\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    conv = conv2d_maxpool(conv, 512, (3, 3), (1, 1), (2, 2), (2, 2), conv_padding='SAME', pool_padding='VALID')\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flt = flatten(conv)\n",
    "    flt = tf.nn.dropout(flt, keep_prob)\n",
    "\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc = fully_conn(flt, 4096)\n",
    "    fc = tf.nn.dropout(fc, keep_prob)\n",
    "    fc = fully_conn(flt, 4096)\n",
    "    fc = tf.nn.dropout(fc, keep_prob)\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    return output(fc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "feature=train_feature_chunk[0]\n",
    "label=train_label_chunk[0]\n",
    "\n",
    "# Inputs\n",
    "ROWS = feature.shape[1]\n",
    "COLS = feature.shape[2]     #Resized image columns size\n",
    "CHANNELS = feature.shape[3] #RGB channels\n",
    "image_shape = (ROWS, COLS, CHANNELS)\n",
    "print(\"Image Shape: {}\".format(image_shape))\n",
    "n_classes = label.shape[1]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (None, image_shape[0], image_shape[1], image_shape[2]), name='x')\n",
    "y = tf.placeholder(tf.float32, shape = (None, n_classes), name='y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name='cost')\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (1, 44)), (1, (2, 5))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate((zip([1,2],[44,5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:   0%|                                                                        | 0/30 [00:00<?, ?Epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Training Accuracy over 200 images: 55.50%, Validation Loss: 0.69185, Validation Accuracy: 51.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:   3%|██▏                                                             | 1/30 [00:22<10:50, 22.43s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Training Accuracy over 200 images: 59.50%, Validation Loss: 0.67270, Validation Accuracy: 58.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:   7%|████▎                                                           | 2/30 [00:44<10:24, 22.32s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, Training Accuracy over 200 images: 55.50%, Validation Loss: 0.67767, Validation Accuracy: 55.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  10%|██████▍                                                         | 3/30 [01:06<10:02, 22.31s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4, Training Accuracy over 200 images: 69.50%, Validation Loss: 0.63938, Validation Accuracy: 65.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  13%|████████▌                                                       | 4/30 [01:29<09:39, 22.28s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5, Training Accuracy over 200 images: 69.50%, Validation Loss: 0.61027, Validation Accuracy: 67.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  17%|██████████▋                                                     | 5/30 [01:51<09:16, 22.26s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6, Training Accuracy over 200 images: 79.50%, Validation Loss: 0.57438, Validation Accuracy: 70.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  20%|████████████▊                                                   | 6/30 [02:13<08:53, 22.23s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7, Training Accuracy over 200 images: 81.50%, Validation Loss: 0.52940, Validation Accuracy: 75.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  23%|██████████████▉                                                 | 7/30 [02:35<08:31, 22.23s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8, Training Accuracy over 200 images: 81.00%, Validation Loss: 0.50048, Validation Accuracy: 74.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  27%|█████████████████                                               | 8/30 [02:57<08:08, 22.22s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9, Training Accuracy over 200 images: 78.00%, Validation Loss: 0.50595, Validation Accuracy: 78.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  30%|███████████████████▏                                            | 9/30 [03:19<07:46, 22.20s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Accuracy over 200 images: 83.00%, Validation Loss: 0.47816, Validation Accuracy: 76.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  33%|█████████████████████                                          | 10/30 [03:41<07:23, 22.19s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Training Accuracy over 200 images: 80.50%, Validation Loss: 0.47821, Validation Accuracy: 77.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  37%|███████████████████████                                        | 11/30 [04:03<07:01, 22.17s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Training Accuracy over 200 images: 80.00%, Validation Loss: 0.47565, Validation Accuracy: 78.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  40%|█████████████████████████▏                                     | 12/30 [04:25<06:38, 22.17s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Training Accuracy over 200 images: 80.50%, Validation Loss: 0.45294, Validation Accuracy: 79.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  43%|███████████████████████████▎                                   | 13/30 [04:48<06:16, 22.17s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Training Accuracy over 200 images: 82.50%, Validation Loss: 0.43424, Validation Accuracy: 79.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  47%|█████████████████████████████▍                                 | 14/30 [05:10<05:54, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Training Accuracy over 200 images: 83.00%, Validation Loss: 0.44002, Validation Accuracy: 80.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  50%|███████████████████████████████▌                               | 15/30 [05:32<05:32, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Training Accuracy over 200 images: 86.00%, Validation Loss: 0.41045, Validation Accuracy: 81.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  53%|█████████████████████████████████▌                             | 16/30 [05:54<05:10, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Training Accuracy over 200 images: 85.50%, Validation Loss: 0.43351, Validation Accuracy: 80.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  57%|███████████████████████████████████▋                           | 17/30 [06:16<04:48, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Training Accuracy over 200 images: 83.00%, Validation Loss: 0.40622, Validation Accuracy: 80.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  60%|█████████████████████████████████████▊                         | 18/30 [06:38<04:25, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Training Accuracy over 200 images: 84.50%, Validation Loss: 0.39510, Validation Accuracy: 81.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  63%|███████████████████████████████████████▉                       | 19/30 [07:00<04:03, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Training Accuracy over 200 images: 81.00%, Validation Loss: 0.41682, Validation Accuracy: 80.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  67%|██████████████████████████████████████████                     | 20/30 [07:23<03:41, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Training Accuracy over 200 images: 86.00%, Validation Loss: 0.39483, Validation Accuracy: 81.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  70%|████████████████████████████████████████████                   | 21/30 [07:45<03:19, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Training Accuracy over 200 images: 89.50%, Validation Loss: 0.37327, Validation Accuracy: 85.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  73%|██████████████████████████████████████████████▏                | 22/30 [08:07<02:57, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Training Accuracy over 200 images: 88.00%, Validation Loss: 0.35101, Validation Accuracy: 84.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  77%|████████████████████████████████████████████████▎              | 23/30 [08:29<02:35, 22.15s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Training Accuracy over 200 images: 88.00%, Validation Loss: 0.35919, Validation Accuracy: 84.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  80%|██████████████████████████████████████████████████▍            | 24/30 [08:51<02:12, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Training Accuracy over 200 images: 88.00%, Validation Loss: 0.34577, Validation Accuracy: 84.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  83%|████████████████████████████████████████████████████▌          | 25/30 [09:13<01:50, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Training Accuracy over 200 images: 87.50%, Validation Loss: 0.36535, Validation Accuracy: 84.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  87%|██████████████████████████████████████████████████████▌        | 26/30 [09:36<01:28, 22.15s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Training Accuracy over 200 images: 87.00%, Validation Loss: 0.34347, Validation Accuracy: 86.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  90%|████████████████████████████████████████████████████████▋      | 27/30 [09:58<01:06, 22.15s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Training Accuracy over 200 images: 88.00%, Validation Loss: 0.35885, Validation Accuracy: 83.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  93%|██████████████████████████████████████████████████████████▊    | 28/30 [10:20<00:44, 22.16s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Training Accuracy over 200 images: 86.00%, Validation Loss: 0.34408, Validation Accuracy: 85.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Running Epochs:  97%|████████████████████████████████████████████████████████████▉  | 29/30 [10:42<00:22, 22.15s/Epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Training Accuracy over 200 images: 87.00%, Validation Loss: 0.36012, Validation Accuracy: 84.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epochs: 100%|███████████████████████████████████████████████████████████████| 30/30 [11:04<00:00, 22.15s/Epoch]\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './dog_vs_cat'\n",
    "\n",
    "train_acc_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_acc_epoch = []\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    #Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in tqdm(range(epochs), desc='Running Epochs', unit='Epoch'):\n",
    "        \n",
    "        num_of_chunk = len(train_feature_chunk)\n",
    "        # Loop over all batches      \n",
    "        for i, (train_feature, train_label) in enumerate(zip(train_feature_chunk, train_label_chunk)):\n",
    "            \n",
    "            chunk_size = train_feature.shape[0]\n",
    "            for start in range(0, chunk_size, batch_size):                \n",
    "                end = min(start + batch_size, chunk_size)\n",
    "                train_feature_batch = train_feature[start: end]\n",
    "                train_label_batch = train_label[start: end]\n",
    "                \n",
    "                #Run optimizer on each batch\n",
    "                sess.run(optimizer, feed_dict={\n",
    "                    x: train_feature_batch,\n",
    "                    y: train_label_batch,\n",
    "                    keep_prob: keep_probability\n",
    "                })\n",
    "                \n",
    "                # Calculate and log training accuracy at end of each epoch, for last 200 train images\n",
    "                if (i == num_of_chunk - 1)&(end==chunk_size):\n",
    "                    assert chunk_size >= 200,  'The No. of train images used for calculating accuracy is less than 200'\n",
    "                    train_acc_epoch.append(sess.run(accuracy, feed_dict={\n",
    "                        x: train_feature[-200:],\n",
    "                        y: train_label[-200:],\n",
    "                        keep_prob: 1.0\n",
    "                    }))\n",
    "    \n",
    "        # Calculate and log validation loss and accuracy at end of each epoch for validation set\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        val_batch_count = 0     \n",
    "        for val_feature, val_label in zip(val_feature_chunk, val_label_chunk):\n",
    "            \n",
    "            chunk_size = val_feature.shape[0]\n",
    "            \n",
    "            for start in range(0, chunk_size, batch_size):\n",
    "                end = min(start + batch_size, chunk_size)\n",
    "                val_feature_batch = val_feature[start:end]\n",
    "                val_label_batch = val_label[start:end]\n",
    "                \n",
    "                val_loss += sess.run(cost, feed_dict={\n",
    "                    x: val_feature_batch,\n",
    "                    y: val_label_batch,\n",
    "                    keep_prob: 1.0\n",
    "                })\n",
    "                \n",
    "                val_acc += sess.run(accuracy, feed_dict={\n",
    "                    x: val_feature_batch,\n",
    "                    y: val_label_batch,\n",
    "                    keep_prob: 1.0\n",
    "                })\n",
    "                val_batch_count += 1         \n",
    "        \n",
    "        val_loss_epoch.append(val_loss/val_batch_count)\n",
    "        val_acc_epoch.append(val_acc/val_batch_count)\n",
    "        \n",
    "        print('Epoch {:>2}, Training Accuracy over 200 images: {:.2f}%, Validation Loss: {:>3.5f}, Validation Accuracy: {:.2f}%'.format(epoch+1, \n",
    "                                                                                                                        train_acc_epoch[epoch]*100,\n",
    "                                                                                                                        val_loss_epoch[epoch],\n",
    "                                                                                                                        val_acc_epoch[epoch]*100))\n",
    "        if val_loss_epoch[epoch] <= 0.05629:\n",
    "            break\n",
    "    # Save model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VNX5+PHPQwgii4AERYksjahA\nCAmkoAEFxVKkVRRBWaIFRQqKSyl+XetCF60FpIgvkKrYKosUFfxZEFRQtlbZUYNIIgFDAEOEsCMh\nz++POwmTZJLMJDOZJc/79ZoXuXfOnDlnLpknZ7nniKpijDHGhJpawS6AMcYY44kFKGOMMSHJApQx\nxpiQZAHKGGNMSLIAZYwxJiRZgDLGGBOSLEAZY4wJSRagjPEDEckUkeuDXQ5jIokFKGOMMSHJApQx\nASQi94hIuoj8KCLvi8jFrvMiIi+KyA8ikiciW0Uk3vVcPxFJE5EjIrJHRMYHtxbGBIcFKGMCRESu\nA54DbgMuAnYB81xP9wGuAS4DGgO3A7mu514DfquqDYF4YHk1FtuYkFE72AUwJoINA15X1Y0AIvIY\ncFBEWgOngYbAFcAXqrrN7XWngfYiskVVDwIHq7XUxoQIa0EZEzgX47SaAFDVozitpBaquhyYBrwM\n7BeRmSJynivprUA/YJeIfCYiV1VzuY0JCRagjAmcbKBV4YGI1AeaAnsAVHWqqnYBOuB09T3sOr9O\nVfsDFwALgfnVXG5jQoIFKGP8J1pE6hY+cALLCBFJFJFzgL8An6tqpoj8XES6iUg0cAw4CZwRkToi\nMkxEGqnqaeAwcCZoNTImiCxAGeM/i4ETbo+rgT8A7wB7gThgsCvtecA/cMaXduF0/U10PXcHkCki\nh4HRQGo1ld+YkCK2YaExxphQZC0oY4wxIcmrACUifUVku+uGw0c9PN9SRFaIyCbXDYf9/F9UY4wx\nNUmFXXwiEgV8C/wCyALWAUNUNc0tzUxgk6pOF5H2wGJVbR2wUhtjjIl43rSgugLpqvqdqv6Ecyd8\n/xJpFGfQF6ARzvRaY4wxptK8WUmiBfC923EW0K1EmmeAZSJyP1Af8Liqs4iMAkYB1K9fv8sVV1zh\na3mNMcaEuQ0bNhxQ1WYVpfMmQImHcyX7BYcAb6jqJNdd72+KSLyqFhR7kepMYCZAcnKyrl+/3ou3\nN8YYE0lEZFfFqbzr4ssCLnE7jqV0F97duO52V9X/AnWBGG8KYIwxxnjiTYBaB7QVkTYiUgfnRsP3\nS6TZDfQGEJF2OAEqx58FNcYYU7NUGKBUNR8YCywFtgHzVfVrEZkgIje5kv0euEdEtgBzgeFqdwAb\nY4ypAq+221DVxTjLuLife8rt5zSgu3+LZowxpiazlSSMMcaEJAtQxhhjQpIFKGOMMSHJApQxxpiQ\nFJYBasZnGazNOFDs3NqMA8z4LCNIJTLGGONvYRmgEmIbMXbOpqIgtTbjAGPnbCIhtlGQS2aMMcZf\nwjJApcTFMG1oEmPnbGLysu2MnbOJaUOTSInzvHiFry0ua6EZY0zwhWWAAidIpXZrydTl6aR2a1lm\ncALfW1zWQjPGmODz6kbdULQ24wBvfb6bB667lLc+382VcU3LDFLuLa7Ubi156/Pd5ba4fE1vjDHG\n/8KyBVXYopk2NIlxfS4vCiYlu+Xc+dLiqkx6Y4wx/hWWAWprVl6xFk1hi2drVl6ZrynZ4iovmPma\n3sasjDHG/8IyQI3uGVeqRZMSF8PonnEe0/va4vI1vY1ZGWOM/0mwFh2vzg0LZ3yWQUJso2JBbW3G\nAbZm5XkMar6mL3zexqyMMaZiIrJBVZMrTFcTAlR1mbxsO1OXp/PAdZcyrs/lwS6OMcaEJG8DVFh2\n8YUiG7Myxhj/8ipAiUhfEdkuIuki8mgZaW4TkTQR+VpE5vi3mKHNxqyMMcb/KuziE5Eo4FvgF0AW\nzhbwQ1ybFBamaQvMB65T1YMicoGq/lBevpHUxWdjVsYY4z1vu/i8uVG3K5Cuqt+5Mp4H9AfS3NLc\nA7ysqgcBKgpOkcZTEEqJiyk34LjfZ/XAdZdacDLGmBK86eJrAXzvdpzlOufuMuAyEVkjIv8Tkb6e\nMhKRUSKyXkTW5+TkVK7EEcLGrIwxpnzeBCjxcK5kv2BtoC3QCxgCvCoijUu9SHWmqiaranKzZs18\nLWvEsDErY4ypmDcBKgu4xO04Fsj2kGaRqp5W1Z3AdpyAZTzwdSUMX1dvN8aYSODNGNQ6oK2ItAH2\nAIOBoSXSLMRpOb0hIjE4XX7f+bOgkcTGrIwxpmIVtqBUNR8YCywFtgHzVfVrEZkgIje5ki0FckUk\nDVgBPKyquYEqdE3k61qCxhgT7mwliTDgPmaVEhdT6rikykx7N8aY6mIrSUQQX8esbFKFMSYSWAsq\nQtmNwMaYUGUtqBoukBsu2n1ZxpjqYAEqQgXyRmDrQjTGVAcLUBEo0DcC231ZxpjqYAEqAlXHjcCB\n7EI0xhiwABWRRveMKxUwUuJiyp1i7mvACWQXYqDTG2PCgwUoA/gWcALdhRjo9MaY8GDTzE213Ajs\n67T3QKc3xgSPTTM3XvN1zKo6uhADmd66BI0JDxagTKUCjq98XUswkOkD3SVoAdAYP1HVoDy6dOmi\npmZYk56jSROW6Zr0HI/H1Z3ePc2kpd9UmHb6p+mlnl+TnqPTP033W3l84Wt5jAk1wHr1Ik5YC8oE\nnK9diIFOX5jG2y7BULtPzCaFmJrCJkmYGqk6JmFMXra9aP+ucX0uD2r5A8lWzze+skkSxpTB12ny\nEFr3iflankCPiVmLzgSKVwFKRPqKyHYRSReRR8tJN1BEVEQqjIzGBEtlugRD6T4xX8vja/6+BjRf\nuzRtEonxWkWDVEAUkAH8DKgDbAHae0jXEFgJ/A9IrihfmyRhwoWvkx4qM4nBl0kbgZ4UUtlJHpOW\nfqOtHvlAJy39ptx0gZ5EYkIfXk6S8CZAXQUsdTt+DHjMQ7opwK+BTy1AmUhSXbPmvP2Cr2x5vM2/\nMD9vA1p1pDeRxdsA5U0XXwvge7fjLNe5IiKSBFyiqh+Ul5GIjBKR9SKyPicnx4u3Nib4Qu0+scqU\nx9f7ynwZ4wr0mJ51CdZc3gQo8XCuaOqfiNQCXgR+X1FGqjpTVZNVNblZs2bel9KYCFaZL/hA5+9L\nQAv0mF64T8KwAFsFFTWxqKCLD2gEHAAyXY+TQDYVdPNZF58xjkB3IYbajcaBHkPzVaA//+oYcwu1\n/0MVwY9jULWB74A2nJ0k0aGc9J9WFJzUApQxIStUv+wCNUYXapNgQrEO/g6yfgtQTl70A77Fmc33\nhOvcBOAmD2ktQBlj/KomzXIM1Tr4sxXr1wAViIcFKGOMN6rryzSUZjmGYh18zb883gYoW0nCGBPS\nAr3WIoTeLMdQq0Nl8vcHC1DGmJAW6Gn1oTjLMdTqEOiZpmXyppkViId18RljAiHcJgyEQx2CNYvP\nVjM3xkSUQK+uXh2rt0dCHcrj7WrmFqCMMcZUK9tuwxhjTFizAGWMMSYkWYAyxhgTkixAGWOMCUkW\noIwxxoQkC1DGGGNCkgUoY4wxIckClDHGmJBkAcoYY0xI8ipAiUhfEdkuIuki8qiH58eJSJqIbBWR\nT0Sklf+LaowxpiapMECJSBTwMnAD0B4YIiLtSyTbhLNJYQKwAHjB3wU1xhhTs3jTguoKpKvqd6r6\nEzAP6O+eQFVXqOpx1+H/gFj/FtMYY0xN402AagF873ac5TpXlruBJZ6eEJFRIrJeRNbn5OR4X0pj\njDE1Tm0v0oiHcx6XQBeRVCAZ6OnpeVWdCcwEZzXzks+fPn2arKwsTp486UWxTCipW7cusbGxREdH\nB7soxpgI4U2AygIucTuOBbJLJhKR64EngJ6qeqoyhcnKyqJhw4a0bt0aEU9x0YQiVSU3N5esrCza\ntGkT7OIYYyKEN11864C2ItJGROoAg4H33ROISBLwCnCTqv5Q2cKcPHmSpk2bWnAKMyJC06ZNreVr\njPGrCgOUquYDY4GlwDZgvqp+LSITROQmV7K/AQ2Af4vIZhF5v4zsKmTBKTzZdTPG+Js3XXyo6mJg\ncYlzT7n9fL2fy2WMMaaGs5Uk3PTq1YulS5cWOzdlyhTuvffecl/XoEEDALKzsxk4cGCZeVe0xf2U\nKVM4fvx40XG/fv04dOiQN0Uv1zPPPMPEiROrnI8xxlSnsA1QMz7LYG3GgWLn1mYcYMZnGZXOc8iQ\nIcybN6/YuXnz5jFkyBCvXn/xxRezYMGCSr9/yQC1ePFiGjduXOn8jDEmnIVtgEqIbcTYOZuKgtTa\njAOMnbOJhNhGlc5z4MCBfPDBB5w65UxCzMzMJDs7mx49enD06FF69+5N586d6dixI4sWLSr1+szM\nTOLj4wE4ceIEgwcPJiEhgdtvv50TJ04UpRszZgzJycl06NCBp59+GoCpU6eSnZ3Ntddey7XXXgtA\n69atOXDAqd/kyZOJj48nPj6eKVOmFL1fu3btuOeee+jQoQN9+vQp9j4V8ZTnsWPH+NWvfkWnTp2I\nj4/n7bffBuDRRx+lffv2JCQkMH78eJ8+V2OMqRRVDcqjS5cuWlJaWlqpc+VZk56jSROW6aSl32jS\nhGW6Jj3Hp9d70q9fP124cKGqqj733HM6fvx4VVU9ffq05uXlqapqTk6OxsXFaUFBgaqq1q9fX1VV\nd+7cqR06dFBV1UmTJumIESNUVXXLli0aFRWl69atU1XV3NxcVVXNz8/Xnj176pYtW1RVtVWrVpqT\nc7YOhcfr16/X+Ph4PXr0qB45ckTbt2+vGzdu1J07d2pUVJRu2rRJVVUHDRqkb775Zqk6Pf300/q3\nv/2t2Lmy8lywYIGOHDmyKN2hQ4c0NzdXL7vssqL6Hjx40ONn5+v1M8bUTMB69SJOhG0LCiAlLobU\nbi2Zujyd1G4tSYmLqXKe7t187t17qsrjjz9OQkIC119/PXv27GH//v1l5rNy5UpSU1MBSEhIICEh\noei5+fPn07lzZ5KSkvj6669JS0srt0yrV6/mlltuoX79+jRo0IABAwawatUqANq0aUNiYiIAXbp0\nITMz06t6lpVnx44d+fjjj3nkkUdYtWoVjRo14rzzzqNu3bqMHDmSd999l3r16nn1HsYYUxVhHaDW\nZhzgrc9388B1l/LW57tLjUlVxs0338wnn3zCxo0bOXHiBJ07dwZg9uzZ5OTksGHDBjZv3syFF15Y\n4X0/nqZe79y5k4kTJ/LJJ5+wdetWfvWrX1WYj/MHh2fnnHNO0c9RUVHk5+eXm1dFeV522WVs2LCB\njh078thjjzFhwgRq167NF198wa233srChQvp27evV+9hjDFVEbYBqnDMadrQJMb1uZxpQ5OKjUlV\nVoMGDejVqxd33XVXsckReXl5XHDBBURHR7NixQp27dpVbj7XXHMNs2fPBuCrr75i69atABw+fJj6\n9evTqFEj9u/fz5IlZ5ctbNiwIUeOHPGY18KFCzl+/DjHjh3jvffe4+qrr65SPcvKMzs7m3r16pGa\nmsr48ePZuHEjR48eJS8vj379+jFlyhQ2b95cpfc2xhhveHUfVCjampXHtKFJRd16KXExTBuaxNas\nvCp39Q0ZMoQBAwYUm9E3bNgwbrzxRpKTk0lMTOSKK64oN48xY8YwYsQIEhISSExMpGvXrgB06tSJ\npKQkOnTowM9+9jO6d+9e9JpRo0Zxww03cNFFF7FixYqi8507d2b48OFFeYwcOZKkpCSvu/MA/vSn\nPxVNhABnWSlPeS5dupSHH36YWrVqER0dzfTp0zly5Aj9+/fn5MmTqCovvvii1+9rjDGVJeV1HwVS\ncnKylrwvaNu2bbRr1y4o5TFVZ9fPGOMNEdmgqskVpQvbLj5jjDGRzQKUMcaYkBRyASpYXY6mauy6\nGWP8LaQCVN26dcnNzbUvuzCjrv2g6tatG+yiGGMiSEjN4ouNjSUrKwvbDj78FO6oa4wx/hJSASo6\nOtp2ZDXGGAN42cUnIn1FZLuIpIvIox6eP0dE3nY9/7mItPZ3QY0xxtQsFQYoEYkCXgZuANoDQ0Sk\nfYlkdwMHVfVS4EXgr/4uqDHGmJrFmxZUVyBdVb9T1Z+AeUD/Emn6A/90/bwA6C22B7gxxpgq8GYM\nqgXwvdtxFtCtrDSqmi8ieUBToNjCeCIyChjlOjwqItsrU+gSYkq+T4Sz+kY2q29kq2n1Bc91buXN\nC70JUJ5aQiXngXuTBlWdCcz04j29JiLrvVkyI1JYfSOb1Tey1bT6QtXq7E0XXxZwidtxLJBdVhoR\nqQ00An6sTIGMMcYY8C5ArQPaikgbEakDDAbeL5HmfeA3rp8HAsvV7rY1xhhTBRV28bnGlMYCS4Eo\n4HVV/VpEJuBs2/s+8Brwpoik47ScBgey0CX4tcswDFh9I5vVN7LVtPpCFeoctO02jDHGmPKE1Fp8\nxhhjTCELUMYYY0JS2AaoipZfijQikikiX4rIZhFZX/Erwo+IvC4iP4jIV27nzheRj0Rkh+vfJsEs\noz+VUd9nRGSP6zpvFpF+wSyjP4nIJSKyQkS2icjXIvKg63xEXuNy6huR11hE6orIFyKyxVXfZ13n\n27iWwNvhWhKvjtd5huMYlGv5pW+BX+BMcV8HDFHVtKAWLIBEJBNIVtWIvclPRK4BjgL/UtV417kX\ngB9V9XnXHyJNVPWRYJbTX8qo7zPAUVWdGMyyBYKIXARcpKobRaQhsAG4GRhOBF7jcup7GxF4jV2r\nB9VX1aMiEg2sBh4ExgHvquo8EZkBbFHV6d7kGa4tKG+WXzJhRlVXUvr+OfdltP6J8wseEcqob8RS\n1b2qutH18xFgG84qNBF5jcupb0RSx1HXYbTrocB1OEvggY/XN1wDlKfllyL2wrsosExENriWjKop\nLlTVveD8wgMXBLk81WGsiGx1dQFGRHdXSa4dD5KAz6kB17hEfSFCr7GIRInIZuAH4CMgAzikqvmu\nJD59V4drgPJqaaUI011VO+OsKn+fq3vIRJ7pQByQCOwFJgW3OP4nIg2Ad4CHVPVwsMsTaB7qG7HX\nWFXPqGoizopDXYF2npJ5m1+4Bihvll+KKKqa7fr3B+A9nItfE+x39eUX9un/EOTyBJSq7nf9khcA\n/yDCrrNrbOIdYLaqvus6HbHX2FN9I/0aA6jqIeBT4EqgsWsJPPDxuzpcA5Q3yy9FDBGp7xpkRUTq\nA32Ar8p/VcRwX0brN8CiIJYl4Aq/qF1uIYKus2sQ/TVgm6pOdnsqIq9xWfWN1GssIs1EpLHr53OB\n63HG3VbgLIEHPl7fsJzFB+CamjmFs8sv/TnIRQoYEfkZTqsJnOWp5kRifUVkLtALZ3n+/cDTwEJg\nPtAS2A0MUtWImFhQRn174XT9KJAJ/LZwfCbciUgPYBXwJVDgOv04zrhMxF3jcuo7hAi8xiKSgDMJ\nIgqn8TNfVSe4vr/mAecDm4BUVT3lVZ7hGqCMMcZEtnDt4jPGGBPhLEAZY4wJSRagjDHGhCQLUMYY\nY0KSBShjjDEhyQKUMcaYkGQByhhjTEiyAGWMMSYkWYAyxhgTkixAGWOMCUkWoIwxxoQkC1DGGGNC\nkgUoY4wxIckClDEeiMinInJQRM4JdlmMqaksQBlTgoi0Bq7G2a/npmp839oVpzKm5rAAZUxpdwL/\nA97g7E6viMi5IjJJRHaJSJ6IrHbtHIqI9BCRtSJySES+F5HhrvOfishItzyGi8hqt2MVkftEZAew\nw3Xu7648DovIBhG52i19lIg8LiIZInLE9fwlIvKyiExyr4SI/D8ReSgQH5Ax1cEClDGl3QnMdj1+\nKSIXus5PBLoAKTi7g/4fUCAiLYElwEtAM5zdUjf78H43A92A9q7jda48zgfmAP8Wkbqu58bh7Mja\nDzgPuAs4jrOT6RARqQUgIjFAb2CuLxU3JpRYgDLGjWub7lY421VvADKAoa4v/ruAB1V1j6qeUdW1\nrq2rhwEfq+pcVT2tqrmq6kuAek5Vf1TVEwCq+pYrj3xVnQScA1zuSjsSeFJVt6tjiyvtF0AeTlAC\nGAx8qqr7q/iRGBM0FqCMKe43wDJVPeA6nuM6FwPUxQlYJV1Sxnlvfe9+ICK/F5Ftrm7EQ0Aj1/tX\n9F7/BFJdP6cCb1ahTMYEnQ3KGuPiGk+6DYgSkX2u0+cAjYGLgJNAHLClxEu/B7qWke0xoJ7bcXMP\nadStDFcDj+C0hL5W1QIROQiI23vFAV95yOct4CsR6QS0AxaWUSZjwoK1oIw562bgDM5YUKLr0Q5Y\nhTMu9TowWUQudk1WuMo1DX02cL2I3CYitUWkqYgkuvLcDAwQkXoicilwdwVlaAjkAzlAbRF5Cmes\nqdCrwB9FpK04EkSkKYCqZuGMX70JvFPYZWhMuLIAZcxZvwFmqepuVd1X+ACm4YwzPQp8iRMEfgT+\nCtRS1d04kxZ+7zq/GejkyvNF4CdgP04X3OwKyrAUZ8LFt8AunFabexfgZGA+sAw4DLwGnOv2/D+B\njlj3nokAoqoVpzLGhAURuQanq6+1qhYEuzzGVIW1oIyJECISDTwIvGrByUSCCgOUiLwuIj+IiKdB\nWVz94FNFJF1EtopIZ/8X0xhTHhFpBxzCmcwxJcjFMcYvvGlBvQH0Lef5G4C2rscoYHrVi2WM8YWq\nblPV+qqaoqqHg10eY/yhwgClqitxBn7L0h/4l+umwf8BjUXkIn8V0BhjTM3kj/ugWlB8llGW69ze\nkglFZBROK4v69et3ueKKK/zw9sYYY8LJhg0bDqhqs4rS+SNAiYdzHqcGqupMYCZAcnKyrl+/3g9v\nb4wxJpyIyC5v0vljFl8WzvIrhWKBbD/ka4wxpgbzR4B6H7jTNZvvSiBPVUt17xljjDG+qLCLT0Tm\nAr2AGBHJAp4GogFUdQawGOcu+nScZf9HBKqwxhhjao4KA5SqDqngeQXu81uJjDHGGGwlCWOMMSHK\nApQxxpiQZAHKGGNMSLIAZYwxJiRZgDLGGBOSLEAZY4wJSRagjDHGhCQLUMYYY0KSBShjjDEhyQKU\nMcaYkGQByhhjTEiyAGWMMSYkWYAyxhgTkvyxo64xxgTWsWMwaRIsWQKJiXD11dCjB7RsGeySmQCy\nAGWMCV1nzsCsWfDUU7B3LyQnw5w5MGOG83zLlk6gKgxY7dtDLesYihQWoIwxoUfVaS393//B11/D\nVVfBggWQkuIErS+/hFWrYPVqWLHCCVoATZpA9+5nA1aXLnDOOcGti6k0C1DGmNCyaRM8/DB88glc\neqkTmAYMABHn+agop5svMRHuv98JZjt3OsGqMGh98EH1lLVWLTj/fIiJKf5o1qz0ucJHw4Zn61JZ\neXmwfTt88w1s2+b8+803sGsXdOsGt9wCN98c9l2g4myIW/2Sk5N1/fr1QXlvY0wI2r0bnnwS3nrL\n+dJ/6ikYPRrq1PE9r5wcWLMGtm51WlyBkp8PBw/CgQPOex44cPaRn+/5NXXqeA5cbkFtxtHGJFzS\nhJRLY5zg+803rP12P1tzf2L0p29BdvbZ/GrXhrZtoV07uOgip0WZluY817mzE6xuucXp/qxKYFSF\n/fvh3HOhUaPK5wOIyAZVTa4wnQUoY0xQ5eXBc8/BlCnO8UMPwaOPQuPGwS1XVajC4cNng1Vh8MrJ\ngdzc4oGs8PzBg87rgLUtOzK2/6NMW/Q8Kbu/dI5vfoxp2xeS0vxcJxhdcYXzb5s2EB1d/P2//RYW\nLnQe//2vc65tW6dVdcstTiurrLG6/HwnKBa2zNxbaIcOweuvw4gRVfp4LEAZE24OH3a6aJo2df6S\nrkzLIZycPu1Mdnj2WedLOzUV/vQnaNWqWLIZn2WQENuIlLiYonNrMw6wNSuP0T3jqrvUgePeGjtw\ngLUZuYzdEUVqqzq8tRemDevitKh8tXcvLFoE770Hy5dDfj4zeg8n4YqLSenTDY4cgW3bWLvrEFvz\nChj94avOtSnUvPnZgHjFFfDLX8Lll1epqhagjAkX+/fDiy/C9OlOkCp03nkVj2tcfrnz5RFusrNh\n0CBYuxauuw7+9jenO8qDtRkHGDtnE9OGJpESF1PquKp8DYCBTu9u8rLtTF2ezgPXXcq4PlULCoDT\nAlq8mLUf/o+xTa5i2sLnnBZa60TG3vwo0/Z/SkrLRmdbZ5dfDo0b+/2PBG8DFKoalEeXLl3UmBrt\nu+9Ux4xRPeccVRHV225TnTNHdfp01T/+UfXBB1WHDVP95S9Vu3RRbdlStV49Vacj6Oyja1fVf/xD\n9ciRYNfIO6tWqTZv7tRl9mzVgoIKX7ImPUeTJizTSUu/0aQJy3RNek6Zaad/ml7q+TXpOTr90/Ry\n8y58Tcnj6k5fmTpXxpq0PZr0h//opNmrNenZpeXmX9k6lAVYr17ECQtQxlS3L790Ak9UlGp0tOrI\nkarffuv9648dU929W3XDBtUXX1Rt3975VW7QQPWee1S/+MKrL/1ynTql+r//qX76adXzKlRQoDp1\nqmrt2qqXXup8Dj6YtPQbbfXIBzpp6TflpqvMl6mvwaC60ntbB1+DciFvP9PK1KE8FqCMCTVr16re\neKPza1e/vuq4capZWVXPt6BAdc0a1eHDVc8918m/UyfVl15SPXjQuzwOH1Zdtkz1D39Q7dXrbD6g\n2r276n//W7UyHjummprq5Hfjjd6XyyXQAUHVty/rQKcPdCvQPU0gP6OyWIAyJhQUFKh++KFqz57O\nr9v556s+84zqgQOBeb9Dh5wuws6dnferW1f1jjtUV64s3hLat091wQKnG7FLF6c1B6q1ajmvffBB\n1X//W3XGDNULL3SeGzRINb38v8g9yshwAqaI6oQJqmfO+PTyynYvBbJ1UB0B01e+vEd1BbSyWIAy\noaWgQPXdd50vxVOngl0a/8rPV/3hB9W0NNXPPlN95x3VV15R/dOfVJOSnF+zFi1UJ0+u3nGiDRuc\nMa7zznPKcPnlTrBq2/Zs6+jcc50W0x/+oLp0qdOSKunIEdWnn3bGjKKjneDlbYBdvFi1SRPVxo2d\nnyuhMt1Xgfyyrq4xqMrwNihXRwutPBagTOg4dswZcyn8UmzWTHX8eNVvqtZNUK2++84JMHffrdq/\nv2pKiupllzktIhEtNXGh8HF+zm46AAAcuUlEQVT55aqvvqp68mSZWVd2/MBrR4+qzprllPmCC1Rv\nukn1b39zuu18+WMhO9sZ46pVS7VRI9UXXlA9ccJz2jNnnIkeIk7rya3lFej6Bnr8JtDpKyuQrTR/\n18EClAkN7t07f/qT6pIlqgMGOAPloHr11ar/+pfq8ePBLmlxBQWqmzc7LYdOnc4GnObNVRMSVK+7\nzpl1d999TpqXXlKdO1f1o49UN21yxpbK+vIuoboGxL1VYf5ffaX6q185n0erVqpvvVW82+7QobNj\nbampzh8oJfIKZIuiugJCKKnOVpo/WICqyY4edbrTRo50Bs7Hj1f9619VX3tNddEiZ0B9+3bV3Fyf\nxwN88p//OF07TZo44zDu9u1zynTppc5/w8aNVceOVd2yJXDlqUh+vjNW87vfqbZp45RLRLVHD9WJ\nEys3/uKlQHZJBaw755NPznZhdumiuny5MzOvbVvnD5CXXipzBmB1jMnUJOEWlP0aoIC+wHYgHXjU\nw/MtgRXAJmAr0K+iPC1A+dmBA043Tv/+Z2dgNW6sGhvr3GdTVhdUVJTT7dO+veo116j+5S8+z7Aq\n5cwZ1Wefdb7cExOd7rGyFBSorlihOnTo2XJ27ao6c6bn8RB/O3FC9YMPnK67Zs2c969TR7VfP6cM\n+/b5nGWoTfkN6ID4mTM6ffJ8XZN8vfPZRUerNm+ua95b4df6msjibYCqcCUJEYkCvgV+AWQB64Ah\nqprmlmYmsElVp4tIe2CxqrYuL19bScIPdu8+u97WypXOopixsWfX27r6ameNLlU4frz0umAlH99/\nD1984axgMGaMsyZa8+a+lenQIbjjDmc16TvucJayqVfPu9f++CO8+Sb84x/OFgsNGkDXrp5XUCh5\nruSWCidOlFnfGYfPI+HH3aR8/yWsXw9Hj7L28m5s7fkrRve+HPr2dT4DF1/voq/MygeFaVK7teSt\nz3d7tUqCL6sMBDL/tRkHGDt7I9OiviXly9WsHTeBsUt3+b2+JnL4bSUJ4CpgqdvxY8BjJdK8Ajzi\nln5tRflaC6oSCgqc/v8//tHpUilsBbVvr/r446rr1lX9psqNG52xFRGnRTNmTPktIHdbtzpddrVr\nq06bVvmyFBQ4A/j33FN8MkJZrcDCm1Rbty57tYXCh4iu6dhDkx6ap2t+NVR11ChdM2dxuXfSB3pK\nbijewxLIadThNl5i/A9/dfEBA4FX3Y7vAKaVSHMR8CVOC+sg0KWMvEYB64H1LVu2rKaPIkIcPqya\nnHz2i/bKK1Wff94ZSwqEb791xrCio51uwGHDiu7899iFNXO+Tu8xWPWii1RXrw5MmU6fdqZzf/11\nsenc05+aqWvGTXAG5O+4Q3XcOF0zYapOf2GO6nvvOUvrbNvmdIPm5zvlrYb7WEJpym91BJBA1ddE\nHn8GqEEeAtRLJdKMA36vZ1tQaUCt8vK1FpSP7r/fadW8+KLqnj3V975ZWc6KB/XrO/9dfv1rXbPo\n07NfWD/9pGvGTdCk+2c7LZLs7Cq/ZXXdoxHIlQBCacpvdcwStEkPxhf+DFDedPF9DVzidvwdcEF5\n+VqA8sF//+sEp7Fjg1eGAwecFRBcXW1rfp2qSU/8P5007HEnOD30tOpPP/nlraqjy6smdWGF2n1H\nxvgzQNV2BZw2QB1gC9ChRJolwHDXz+2AbFxbeZT1sADlpZ9+Uo2Pd1YiyMsLdmmcKewvvqjaooVO\n6jHMaVFMWuD3twlkl5qvX6jV0QIJZzWtvqbq/BagnLzohzOTLwN4wnVuAnCT6+f2wBpX8NoM9Kko\nTwtQXvrzn53LtGhRsEtSzJptezXp8fd10lurKgwgoTbtOlRXAjCmpvBrgArEwwKUF7791plJd+ut\nwS5JMYFugbinCccuNWNM+bwNULajbqhShd69YeNG2LYNLrrI65cGeovsyuTvy30vvt5HVGO2BDcm\nQnh7H1St6iiMqYQ33oAVK+CFF3wKTgAJsY0YO2cTazMOAGe/8BNiG3lMP+OzjKK0hdZmHGDGZxke\n04/uGVcqUKTExZQbDFLiYkjt1pKpy9NJ7day3Jsyt2blFQtGKXExTBuaxNasPL+VxxgT+ixAhaL9\n++H3v3dWghg50ueXF36hj52zicnLtle4ioGvAa0y1mYc4K3Pd/PAdZfy1ue7SwVEdxZwjDHgzNAz\noeZ3v4Njx+CVV6BW5f6GcG+xPHDdpeW2WNwDWiCWninZRXdlXNMKg6YxxlgLKtQsWQJz58Ljj0O7\ndpXOxpcWC/jWBecrX7vsjDEGsEkSIeXoUYiPdxZX3bSp9AKoXqquxUqNMaYybJJEOHrqKdi1C2bO\nrHRwAt9bLO4BbFyfy4u6+ypqdRljTCBZCypUrF8P3brBqFEwfXq1vrVN0zbGVCdvW1AWoELB6dPO\nvkf79zv3PDUqPnvOAogxJpJYF184mTIFNm+GadNKBSeonmngxhgTaqwFFWzffedMjPjlL+G998pM\nZpMYjDGRwlpQ4UAVRo+G2rWd1lM5AjkN3BhjQpEFqGB66y346CN4/nlo0aLcpL7e12SMMeHOVpLw\nl9xcZ2miH37w/jV5eXDVVU4rqhy2EoMJdadPnyYrK4uTJ08GuygmhNStW5fY2Fiio6Mr9XoLUP4y\nfbozA2/UKKjgYsyIbkNCQR4ptY/Bgw9CrVrlzsor774mC1AmFGRlZdGwYUNat26NiAS7OCYEqCq5\nublkZWXRpk2bSuVhAcofTp6El16CG25w1s+rQIJ7i6hl8ZUePPEUtFLiYiw4mZBx8uRJC06mGBGh\nadOm5OTkVDoPC1D+8OabTtfeww97lTzQi7MaEwwWnExJVf0/YZMkqqqgACZNgs6doVcvr19ms/KM\nMaZ8FqCq6j//YUajeNaOfgTc/loob8O/wudtVp4x/pGbm0tiYiKJiYk0b96cFi1aFB3/9NNPXuUx\nYsQItm/fXm6al19+mdmzZ/ujyADs37+f2rVr89prr/ktz0hiXXxVNXEiCQV1GLu3MdMyDpRaPdwT\nm5VnjH81bdqUzZs3A/DMM8/QoEEDxo8fXyyNqqKq1Cpjj7VZs2ZV+D733Xdf1Qvr5u233+aqq65i\n7ty53H333X7N211+fj61a4ff1334lTiUfPEFrFxJyuTJTLups9djSjYrz0S0hx5ylu7yp8REZ0kw\nH6Wnp3PzzTfTo0cPPv/8cz744AOeffZZNm7cyIkTJ7j99tt56qmnAOjRowfTpk0jPj6emJgYRo8e\nzZIlS6hXrx6LFi3iggsu4MknnyQmJoaHHnqIHj160KNHD5YvX05eXh6zZs0iJSWFY8eOceedd5Ke\nnk779u3ZsWMHr776KomJiaXKN3fuXKZNm8agQYPYt28fzZs3B+A///kPf/jDHzhz5gwXXnghy5Yt\n48iRI4wdO5aNGzciIkyYMIFf//rXxMTEcOjQIQDmzZvHxx9/zKuvvkpqaioXXnghGzdu5Oc//zkD\nBgzgd7/7HSdPnqRevXq88cYbtG3blvz8fB5++GE++ugjatWqxejRo4mLi+PVV1/l3//+NwBLlixh\n1qxZzJ8/v7JXsFIsQFXFpEnO2nkjR5LSsKHXO9jarDxjqk9aWhqzZs1ixowZADz//POcf/755Ofn\nc+211zJw4EDat29f7DV5eXn07NmT559/nnHjxvH666/z6KOPlspbVfniiy94//33mTBhAh9++CEv\nvfQSzZs355133mHLli107tzZY7kyMzM5ePAgXbp0YeDAgcyfP58HHniAffv2MWbMGFatWkWrVq34\n8ccfAadl2KxZM7788ktUtSgolScjI4NPPvmEWrVqkZeXx+rVq4mKiuLDDz/kySef5O2332b69Olk\nZ2ezZcsWoqKi+PHHH2ncuDEPPPAAubm5NG3alFmzZjFixAhfP/oqswBVWTt3woIFMH48NGxYakzp\nyrimFnBMzVSJlk4gxcXF8fOf/7zoeO7cubz22mvk5+eTnZ1NWlpaqQB17rnncsMNNwDQpUsXVq1a\n5THvAQMGFKXJzMwEYPXq1TzyyCMAdOrUiQ4dOnh87dy5c7n99tsBGDx4MPfddx8PPPAA//3vf7n2\n2mtp1aoVAOeffz4AH3/8MQsXLgSc2XFNmjQhPz+/3LoPGjSoqEvz0KFD3HnnnWRkFB8b//jjj3no\noYeIiooq9n5Dhw5lzpw5DBs2jA0bNjB37txy3ysQLEBV1pQpUKsWPPCAjSkZE8Lq169f9POOHTv4\n+9//zhdffEHjxo1JTU31uPpFnTp1in6OiooqMxCc49pY1D2Ntwtwz507l9zcXP75z38CkJ2dzc6d\nO1FVj9OzPZ2vVatWsfcrWRf3uj/xxBP88pe/5N577yU9PZ2+ffuWmS/AXXfdxa233grA7bffXhTA\nqpPN4quMH3+E116DoUOhRQufd7A1xgTH4cOHadiwIeeddx579+5l6dKlfn+PHj16FI3VfPnll6Sl\npZVKk5aWxpkzZ9izZw+ZmZlkZmby8MMPM2/ePLp3787y5cvZtWsXQFEXX58+fZjmWlRaVTl48CC1\natWiSZMm7Nixg4KCAt4rZ0eEvLw8WrjW/HzjjTeKzvfp04fp06dz5syZYu93ySWXEBMTw/PPP8/w\n4cOr9qFUkgWoynjlFTh2zOnewxlTKtlSSomLsc0EjQkxnTt3pn379sTHx3PPPffQvXt3v7/H/fff\nz549e0hISGDSpEnEx8fTqMQ+b3PmzOGWW24pdu7WW29lzpw5XHjhhUyfPp3+/fvTqVMnhg0bBsDT\nTz/N/v37iY+PJzExsajb8a9//St9+/ald+/exMbGllmuRx55hIcffrhUnX/729/SvHlzEhIS6NSp\nU7GJEEOHDqVNmzZcdtllVfpMKsv2g/LVqVPQujV06gQffhjs0hgTErZt20a7du2CXYyQkJ+fT35+\nPnXr1mXHjh306dOHHTt2hOU079GjR3PVVVfxm9/8ptJ5ePq/4e1+UOH3iQXbnDmwb5+zvJExxpRw\n9OhRevfuTX5+PqrKK6+8EpbBKTExkSZNmjB16tSglSH8PrVgUoWJE53WU+/ewS6NMSYENW7cmA0b\nNgS7GFW22d/3slWCV2NQItJXRLaLSLqIlL4ZwElzm4ikicjXIjLHv8UMEUuWQFqaM/ZkC2MaY0xA\nVdiCEpEo4GXgF0AWsE5E3lfVNLc0bYHHgO6qelBELghUgYNq4kRn51vXvQvGGGMCx5sWVFcgXVW/\nU9WfgHlA/xJp7gFeVtWDAKrqw7aygTfjs4xSi7FWtJhrKRs2wIoVzjIuldwd0hhjjPe8CVAtgO/d\njrNc59xdBlwmImtE5H8i0tdTRiIySkTWi8j6qmxi5auEixsy9rW1rE3uDU8+ydpt2Yyds4mE2EYV\nv7jQpEnQsCHcc0/gCmqMMaaINwHK02BLybnptYG2QC9gCPCqiDQu9SLVmaqarKrJzZo187WslbN5\nMynDfs202U8y9upRTP4sk7EzVzGtHd6v8rBrF8yf72zn3siHoGaMKcUvPRol9OrVq9RNt1OmTOHe\ne+8t93UNGjQAnFUcBg4cWGbeFd0SM2XKFI4fP1503K9fP6/WyvNWp06dGDJkiN/yCxfeBKgs4BK3\n41gg20OaRap6WlV3AttxAlbwHDsG//d/kJwMu3aR8vxjpN6QyNTuQ0jNWE3KoD4wfDgc8GIfpr//\n3ZkU8eCDAS+2MZEuIbYRY+dsKgpShUuF+dSjUcKQIUOYN29esXPz5s3z+kv94osvZsGCBZV+/5IB\navHixTRuXOpv9ErZtm0bBQUFrFy5kmPHjvklT08qWtcvGLwJUOuAtiLSRkTqAIOB90ukWQhcCyAi\nMThdft/5s6A++fBDiI+Hv/3NCULbtrG26y/OLuaafCNrH3seZs+GK65w7mkq64blQ4fgH/+AwYPh\nkks8pzHGeK1wKbCxczYxedl2v6xbOXDgQD744ANOnToFOCuFZ2dn06NHj6L7kjp37kzHjh1ZtGhR\nqddnZmYSHx8PwIkTJxg8eDAJCQncfvvtnDhxoijdmDFjSE5OpkOHDjz99NMATJ06lezsbK699lqu\nvfZaAFq3bs0B1x+/kydPJj4+nvj4eKa4FtLNzMykXbt23HPPPXTo0IE+ffoUex93c+bM4Y477qBP\nnz68//7Zr9709HSuv/56OnXqROfOnYsWgX3hhRfo2LEjnTp1KlqB3b0VeODAAVq3bg04Sx4NGjSI\nG2+8kT59+pT7Wf3rX/8qWm3ijjvu4MiRI7Rp04bTp08DzjJSrVu3Ljr2i8JNvMp7AP2Ab4EM4AnX\nuQnATa6fBZgMpAFfAoMryrNLly7qd/v2qQ4erAqql1+u+tlnqqq6Jj1HkyYs0zXpOcWPl32heuWV\nTvrevVV37Cid51//6jy/aZP/y2tMhEhLS/P5NZOWfqOtHvlAJy39xi9l6Nevny5cuFBVVZ977jkd\nP368qqqePn1a8/LyVFU1JydH4+LitKCgQFVV69evr6qqO3fu1A4dOjjlmjRJR4wYoaqqW7Zs0aio\nKF23bp2qqubm5qqqan5+vvbs2VO3bNmiqqqtWrXSnJycorIUHq9fv17j4+P16NGjeuTIEW3fvr1u\n3LhRd+7cqVFRUbrJ9b0yaNAgffPNNz3Wq23btpqZmalLly7VG2+8seh8165d9d1331VV1RMnTuix\nY8d08eLFetVVV+mxY8eKlbdnz55FdcjJydFWrVqpquqsWbO0RYsWRenK+qy++uorveyyy4rqWJh+\n+PDh+t5776mq6iuvvKLjxo0rVX5P/zeA9epF7PHqPihVXayql6lqnKr+2XXuKVV93/Wzquo4VW2v\nqh1VdV75OfpZQQG8+qrTGnr3XXjmGdiyBa65Bihng8A658OaNfDyy7BuHXTsCH/5CzM++dbpfvjp\nJ6d77/rrWdswtkp95MaYs0puT1NyTKoy3Lv53Lv3VJXHH3+chIQErr/+evbs2cP+/fvLzGflypWk\npqYCkJCQQEJCQtFz8+fPp3PnziQlJfH11197XAjW3erVq7nllluoX78+DRo0YMCAAUVr6LVp06Zo\nE0P37TrcrVu3jmbNmtGqVSt69+7Nxo0bOXjwIEeOHGHPnj1F6/nVrVuXevXq8fHHHzNixAjq1asH\nnN06ozy/+MUvitKV9VktX76cgQMHEhMTUyzfkSNHFu1EHIg9o8J/JYlt2+C3v4VVq5yA9MorTqBy\nU+EGgffeCzffDA88AE88QULPzxl7zW+ZdtEhUrKzWTvp1XK3cDfGeC9Q29PcfPPNjBs3rmi33MKN\nAmfPnk1OTg4bNmwgOjqa1q1be9xiw52n7Sd27tzJxIkTWbduHU2aNGH48OEV5qPlrHVauFUHONt1\neOrimzt3Lt98801Rl9zhw4d55513uO2228p8P09lr127NgUFBUD5W3KU9VmVlW/37t3JzMzks88+\n48yZM0XdpP4SvquZnzrltJQ6dYKvvnJaUCtWlApOXrv4YmcDwkWLSMnYyLR/PcbYb4TJA37H2B21\nbW8nY/wkUNvTNGjQgF69enHXXXcVmxyRl5fHBRdcQHR0NCtWrCjaxqIs11xzDbNnzwbgq6++YuvW\nrYATHOrXr0+jRo3Yv38/S5YsKXpNw4YNOXLkiMe8Fi5cyPHjxzl27BjvvfceV199tVf1KSgo4N//\n/jdbt24t2pJj0aJFzJ07l/POO4/Y2NiiDQxPnTrF8ePH6dOnD6+//nrRhI3CrTNat25dtPxSeZNB\nyvqsevfuzfz588nNzS2WL8Cdd97JkCFDArLjbvgGqDVr4NlnYeBApxV1993OBoJVddNNkJZGys3X\nkrppMVPb9ia1W0sLTsb4SSC3pxkyZAhbtmxh8ODBReeGDRvG+vXrSU5OZvbs2VxRwR+xY8aM4ejR\noyQkJPDCCy/QtWtXwJnqnZSURIcOHbjrrruKbVsxatQobrjhhqJJEoU6d+7M8OHD6dq1K926dWPk\nyJEkJXnXE7Ny5UpatGhRtIcTOAEvLS2NvXv38uabbzJ16lQSEhJISUlh37599O3bl5tuuonk5GQS\nExOZOHEiAOPHj2f69OmkpKQUTd7wpKzPqkOHDjzxxBP07NmTTp06MW7cuGKvOXjwYECmwYf3dhtb\ntjgtqABYm3GAsbM3knplK976fLe1oIwph223UXMtWLCARYsW8WYZOzzU3O02Ahmc5mxi2rDOtoW7\nMcaU4f7772fJkiUsXrw4IPmHd4AKkPL6yC1AGWOM46WXXgpo/hagPKhw1p8xppSyZnqZmquqQ0jh\nO0nCGBMy6tatS25ubpW/kEzkUFVyc3OpW7dupfMIyxbUjM8ySIhtVKxFszbjAFuz8vwyE8gY45vY\n2FiysrKozl0KTOirW7cusbGxlX59WAaowsUmC8eJ3G/8M8ZUv+joaNq0aRPsYpgIE5YByn2xydRu\nLW0auDHGRKCwHYNKiYshtVtLpi5PtxtpjTEmAoVtgArEYpPGGGNCR9BWkhCRHKD8RbHKeu059RrW\nbtT8Z/l5+77TU8fPkXPqnXI7Lr0gVmSJAWpSNLb6Rjarb+TzVOdWqlrhtupBC1D+IiLrvVkyI1JY\nfSOb1Tey1bT6QtXqHLZdfMYYYyKbBShjjDEhKRIC1MxgF6CaWX0jm9U3stW0+kIV6hz2Y1DGGGMi\nUyS0oIwxxkQgC1DGGGNCUtgGKBHpKyLbRSRdRB4NdnkCTUQyReRLEdksIlXcijg0icjrIvKDiHzl\ndu58EflIRHa4/m0SzDL6Uxn1fUZE9riu82YR6RfMMvqTiFwiIitEZJuIfC0iD7rOR+Q1Lqe+EXmN\nRaSuiHwhIltc9X3Wdb6NiHzuur5vi0gdr/MMxzEoEYkCvgV+AWQB64AhqpoW1IIFkIhkAsmqGrE3\n+YnINcBR4F+qGu869wLwo6o+7/pDpImqPhLMcvpLGfV9BjiqqhODWbZAEJGLgItUdaOINAQ2ADcD\nw4nAa1xOfW8jAq+xOJuB1VfVoyISDawGHgTGAe+q6jwRmQFsUdXp3uQZri2orkC6qn6nqj8B84D+\nQS6TqSJVXQn8WOJ0f+Cfrp//ifMLHhHKqG/EUtW9qrrR9fMRYBvQggi9xuXUNyKp46jrMNr1UOA6\nYIHrvE/XN1wDVAvge7fjLCL4wrsosExENojIqGAXphpdqKp7wfmFBy4Icnmqw1gR2erqAoyI7q6S\nRKQ1kAR8Tg24xiXqCxF6jUUkSkQ2Az8AHwEZwCFVzXcl8em7OlwDlKd9pcOvr9I33VW1M3ADcJ+r\ne8hEnulAHJAI7AUmBbc4/iciDYB3gIdU9XCwyxNoHuobsddYVc+oaiIQi9PT1c5TMm/zC9cAlQVc\n4nYcC2QHqSzVQlWzXf/+ALyHc/Frgv2uvvzCPv0fglyegFLV/a5f8gLgH0TYdXaNTbwDzFbVd12n\nI/Yae6pvpF9jAFU9BHwKXAk0FpHCvQd9+q4O1wC1Dmjrmh1SBxgMvB/kMgWMiNR3DbIiIvWBPsBX\n5b8qYrwP/Mb182+ARUEsS8AVflG73EIEXWfXIPprwDZVnez2VERe47LqG6nXWESaiUhj18/nAtfj\njLutAAa6kvl0fcNyFh+Aa2rmFCAKeF1V/xzkIgWMiPwMp9UEzi7IcyKxviIyF+iFszz/fuBpYCEw\nH2gJ7AYGqWpETCwoo769cLp+FMgEfls4PhPuRKQHsAr4EihwnX4cZ1wm4q5xOfUdQgReYxFJwJkE\nEYXT+JmvqhNc31/zgPOBTUCqqp7yKs9wDVDGGGMiW7h28RljjIlwFqCMMcaEJAtQxhhjQpIFKGOM\nMSHJApQxxpiQZAHKGGNMSLIAZYwxJiT9f17EqJZaKqg0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aed14ff438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot = plt.subplot(211)\n",
    "loss_plot.set_title('Loss')\n",
    "loss_plot.plot(range(epoch+1), val_loss_epoch, 'x', label='Validation Loss')\n",
    "loss_plot.legend(loc='lower left')\n",
    "loss_plot.set_ylim([0, 0.8])\n",
    "\n",
    "acc_plot = plt.subplot(212)\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(range(epoch+1), train_acc_epoch, 'r', label='Training Accuracy')\n",
    "acc_plot.plot(range(epoch+1), val_acc_epoch, 'x', label='Validation Accuracy')\n",
    "acc_plot.legend(loc='lower right')\n",
    "acc_plot.set_ylim([0.5,1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Point 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './dog_vs_cat'\n",
    "\n",
    "\n",
    "preprocess_test_images = [i for i in os.listdir() if 'preprocess_test' in i]\n",
    "test_feature_in_chunk = []\n",
    "test_id_in_chunk = []\n",
    "for preprocess_test in tqdm(preprocess_test_images, desc='Reload Test Image Chunks', unit='Chunk'):\n",
    "    with open(preprocess_test, 'rb') as f:\n",
    "        pickle_chunk_data=pickle.load(f)\n",
    "        test_feature_in_chunk.append(pickle_chunk_data['test_features'])\n",
    "        test_id_in_chunk.append(pickle_chunk_data['test_ids'])\n",
    "        del pickle_chunk_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "predictions = []\n",
    "    \n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    #Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "        \n",
    "    # Get Tensors from loaded model\n",
    "    loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        \n",
    "    for test_feature in tqdm(test_feature_in_chunk, desc='Making Prediction', unit='data chunk'):\n",
    "        for batch_start in range(0, test_feature.shape[0], batch_size):\n",
    "            batch_end = min(batch_start + batch_size, test_feature.shape[0])\n",
    "            test_feature_batch = test_feature[batch_start: batch_end]\n",
    "            test_predictions = sess.run(\n",
    "            tf.nn.softmax(loaded_logits),\n",
    "            feed_dict={loaded_x: test_feature_batch, loaded_keep_prob: 1.0})\n",
    "            predictions.append(test_predictions[:,0])\n",
    "    \n",
    "    predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids=[image_id for chunk_id in test_id_in_chunk for image_id in chunk_id]\n",
    "    \n",
    "submissions = {'id': test_ids,\n",
    "                'label': predictions}\n",
    "df = pd.DataFrame.from_dict(submissions)\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
